{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Análise e Comparação dos Resultados\n",
    "\n",
    "Este notebook foca na análise quantitativa e comparação dos resultados obtidos pelos métodos Geométrico e de Machine Learning. Ele utiliza as funções do módulo `metrics.py` para calcular erros e taxas de detecção.\n",
    "\n",
    "**Pré-requisitos:**\n",
    "1. Resultados da detecção (arquivos JSON) gerados para ambos os métodos (Geométrico e ML) para um conjunto de dados. Idealmente, gerados usando o script `src/main.py` em modo batch.\n",
    "2. Arquivos JSON contendo as coordenadas ground truth (GT) para os landmarks correspondentes aos arquivos processados.\n",
    "\n",
    "**Objetivos:**\n",
    "1. Carregar os resultados previstos e os dados ground truth.\n",
    "2. Utilizar `run_evaluation_on_dataset` para calcular métricas (Erro por landmark, MDE, Taxa de Detecção) para cada método.\n",
    "3. Gerar DataFrames com os resultados detalhados e agregados.\n",
    "4. Criar visualizações (gráficos) para comparar a performance dos métodos (ex: boxplots de erro por landmark, gráfico de barras de taxa de detecção).\n",
    "5. Analisar a complexidade computacional (tempo de execução), se os tempos foram registrados durante a execução em lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações Iniciais e Imports\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Para gráficos mais elaborados\n",
    "\n",
    "# Adicionar o diretório raiz do projeto ao path\n",
    "module_path = os.path.abspath(os.path.join(".."))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.utils.metrics import run_evaluation_on_dataset, load_landmarks_from_json\n",
    "from src.utils.helpers import setup_logging, list_stl_files # Para obter lista de arquivos base\n",
    "from src.core.landmarks import LANDMARK_NAMES\n",
    "\n",
    "# Configurar logging\n",
    "setup_logging(log_level=logging.INFO)\n",
    "\n",
    "# Diretórios (ajuste conforme necessário)\n",
    "BASE_DIR = module_path\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, "results")\n",
    "# Assumir que os resultados estão em subpastas por método\n",
    "RESULTS_GEOM_DIR = os.path.join(RESULTS_DIR, "geometric") \n",
    "RESULTS_ML_DIR = os.path.join(RESULTS_DIR, "ml")\n",
    "# Diretório com os ground truths (IMPORTANTE: Precisa existir com dados GT!)\n",
    "GROUND_TRUTH_DIR = os.path.join(BASE_DIR, "data", "ground_truth") # Exemplo\n",
    "DATA_DIR = os.path.join(BASE_DIR, "data", "skulls") # Para obter lista de arquivos base\n",
    "\n",
    "# Criar diretórios se não existirem (especialmente GT para dummy data)\n",
    "os.makedirs(RESULTS_GEOM_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_ML_DIR, exist_ok=True)\n",
    "os.makedirs(GROUND_TRUTH_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# --- IMPORTANTE: Criação de Dados Dummy para Análise ---\n",
    "# Como não temos resultados reais e GT, vamos criar arquivos dummy.\n",
    "# Em um cenário real, estes arquivos seriam gerados pelo main.py e fornecidos.\n",
    "DUMMY_FILE_IDS = ["dummy_A", "dummy_B"]\n",
    "for file_id in DUMMY_FILE_IDS:\n",
    "    # Criar arquivo STL dummy (se não existir)\n",
    "    dummy_stl_path = os.path.join(DATA_DIR, f"{file_id}.stl")\n",
    "    if not os.path.exists(dummy_stl_path):\n",
    "        trimesh.primitives.Sphere().export(dummy_stl_path)\n",
    "        \n",
    "    # 1. Criar Ground Truth Dummy\n",
    "    gt_path = os.path.join(GROUND_TRUTH_DIR, f"{file_id}_landmarks_gt.json")\n",
    "    if not os.path.exists(gt_path):\n",
    "        gt_data = {name: (np.random.rand(3) * 100).tolist() for name in LANDMARK_NAMES} # Coords aleatórias\n",
    "        # Simular alguns landmarks não presentes no GT\n",
    "        if file_id == "dummy_B": gt_data["Nasion"] = None \n",
    "        with open(gt_path, "w") as f: json.dump(gt_data, f, indent=4)\n",
    "            \n",
    "    # Carregar GT para usar como base para as predições dummy\n",
    "    gt_loaded = load_landmarks_from_json(gt_path)\n",
    "    if not gt_loaded: continue # Pular se GT não carregar\n",
    "        \n",
    "    # 2. Criar Resultados Geom Dummy (com algum erro e falhas)\n",
    "    geom_pred_path = os.path.join(RESULTS_GEOM_DIR, f"{file_id}_landmarks.json")\n",
    "    if not os.path.exists(geom_pred_path):\n",
    "        geom_pred = {}\n",
    "        for name, gt_coord in gt_loaded.items():\n",
    "            if gt_coord is not None and np.random.rand() > 0.2: # 80% de chance de detectar\n",
    "                error = (np.random.rand(3) - 0.5) * 10 # Erro entre -5 e 5 mm\n",
    "                geom_pred[name] = (np.array(gt_coord) + error).tolist()\n",
    "            else:\n",
    "                geom_pred[name] = None # Falha na detecção\n",
    "        with open(geom_pred_path, "w") as f: json.dump(geom_pred, f, indent=4)\n",
    "            \n",
    "    # 3. Criar Resultados ML Dummy (com erro menor e menos falhas)\n",
    "    ml_pred_path = os.path.join(RESULTS_ML_DIR, f"{file_id}_landmarks.json")\n",
    "    if not os.path.exists(ml_pred_path):\n",
    "        ml_pred = {}\n",
    "        for name, gt_coord in gt_loaded.items():\n",
    "            if gt_coord is not None and np.random.rand() > 0.1: # 90% de chance de detectar\n",
    "                error = (np.random.rand(3) - 0.5) * 5 # Erro entre -2.5 e 2.5 mm\n",
    "                ml_pred[name] = (np.array(gt_coord) + error).tolist()\n",
    "            else:\n",
    "                ml_pred[name] = None # Falha na detecção\n",
    "        with open(ml_pred_path, "w") as f: json.dump(ml_pred, f, indent=4)\n",
    "\n",
    "logging.info(\"Dados dummy de resultados e GT criados/verificados.\")"\n   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Executar Avaliação para Cada Método\n",
    "\n",
    "Utilizamos a função `run_evaluation_on_dataset` para processar os arquivos de resultado e ground truth, gerando DataFrames com as métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação do Método Geométrico\n",
    "logging.info(\"Avaliando Método Geométrico...\")\n",
    "results_geom_df, summary_geom_df = run_evaluation_on_dataset(\n",
    "    results_dir=RESULTS_GEOM_DIR, \n",
    "    ground_truth_dir=GROUND_TRUTH_DIR, \n",
    "    method_name=\"Geometric\"\n",
    ")\n",
    "\n",
    "# Avaliação do Método ML\n",
    "logging.info(\"Avaliando Método Machine Learning...\")\n",
    "results_ml_df, summary_ml_df = run_evaluation_on_dataset(\n",
    "    results_dir=RESULTS_ML_DIR, \n",
    "    ground_truth_dir=GROUND_TRUTH_DIR, \n",
    "    method_name=\"ML\"\n",
    ")\n",
    "\n",
    "# Combinar resultados detalhados para facilitar comparações\n",
    "if not results_geom_df.empty and not results_ml_df.empty:\n",
    "    results_combined_df = pd.concat([results_geom_df, results_ml_df], ignore_index=True)\n",
    "    print(\"\n--- Resultados Detalhados Combinados ---\")\n",
    "    print(results_combined_df.head().to_markdown(index=False)) # Mostrar início em Markdown\n",
    "elif not results_geom_df.empty:\n",
    "    results_combined_df = results_geom_df\n",
    "    print(\"Apenas resultados geométricos disponíveis para análise detalhada.\")\n",
    "elif not results_ml_df.empty:\n",
    "    results_combined_df = results_ml_df\n",
    "    print(\"Apenas resultados ML disponíveis para análise detalhada.\")\n",
    "else:\n",
    "    results_combined_df = pd.DataFrame()\n",
    "    print(\"Nenhum resultado de avaliação disponível.\")\n",
    "\n",
    "# Combinar sumários\n",
    "if not summary_geom_df.empty:\n",
    "    summary_geom_df[\"Method\"] = \"Geometric\"\n",
    "if not summary_ml_df.empty:\n",
    "    summary_ml_df[\"Method\"] = \"ML\"\n",
    "    \n",
    "if not summary_geom_df.empty and not summary_ml_df.empty:\n",
    "    summary_combined_df = pd.concat([summary_geom_df, summary_ml_df], ignore_index=True)\n",
    "    print(\"\n--- Sumário Comparativo dos Métodos ---\")\n",
    "    print(summary_combined_df.round(2).to_markdown(index=False)) # Arredondar e mostrar em Markdown\n",
    "elif not summary_geom_df.empty:\n",
    "    summary_combined_df = summary_geom_df\n",
    "    print(\"Apenas sumário geométrico disponível.\")\n",
    "elif not summary_ml_df.empty:\n",
    "    summary_combined_df = summary_ml_df\n",
    "    print(\"Apenas sumário ML disponível.\")\n",
    "else:\n",
    "    summary_combined_df = pd.DataFrame()\n",
    "    print(\"Nenhum sumário de avaliação disponível.\")\n",
    "\n",
    "# Salvar DataFrames combinados (opcional)\n",
    "if not results_combined_df.empty:\n",
    "    results_combined_df.to_csv(os.path.join(RESULTS_DIR, \"evaluation_combined_detailed.csv\"), index=False)\n",
    "if not summary_combined_df.empty:\n",
    "    summary_combined_df.to_csv(os.path.join(RESULTS_DIR, \"evaluation_combined_summary.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualização Comparativa das Métricas\n",
    "\n",
    "Criamos gráficos para visualizar e comparar a performance dos dois métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_combined_df.empty:\n",
    "    # Gráfico 1: Boxplot do Erro de Detecção por Landmark e Método\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    sns.boxplot(data=results_combined_df, x=\"Landmark\", y=\"Error\", hue=\"Method\")\n",
    "    plt.title(\"Erro de Detecção (Distância Euclidiana mm) por Landmark e Método\")\n",
    "    plt.ylabel(\"Erro (mm)\")\n",
    "    plt.xlabel(\"Landmark\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    boxplot_path = os.path.join(RESULTS_DIR, \"comparison_error_boxplot.png\")\n",
    "    plt.savefig(boxplot_path)\n",
    "    print(f\"Gráfico Boxplot de Erro salvo em: {boxplot_path}\")\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "else:\n",
    "    print(\"Dados de resultados não disponíveis para gerar boxplot de erro.\")\n",
    "\n",
    "if not summary_combined_df.empty:\n",
    "    # Gráfico 2: Gráfico de Barras da Taxa de Detecção Média por Método\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Calcular taxa média por método\n",
    "    avg_detection_rate = summary_combined_df.groupby(\"Method\")[\"DetectionRate\"].mean().reset_index()\n",
    "    sns.barplot(data=avg_detection_rate, x=\"Method\", y=\"DetectionRate\")\n",
    "    plt.title(\"Taxa de Detecção Média (%) por Método\")\n",
    "    plt.ylabel(\"Taxa de Detecção Média (%)\")\n",
    "    plt.xlabel(\"Método\")\n",
    "    plt.ylim(0, 105)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    # Adicionar valores nas barras\n",
    "    for index, row in avg_detection_rate.iterrows():\n",
    "        plt.text(row.name, row.DetectionRate + 1, f\"{row.DetectionRate:.1f}%\", color=\"black\", ha=\"center\")\n",
    "    plt.tight_layout()\n",
    "    detrate_bar_path = os.path.join(RESULTS_DIR, \"comparison_detection_rate_barplot.png\")\n",
    "    plt.savefig(detrate_bar_path)\n",
    "    print(f\"Gráfico de Barras da Taxa de Detecção salvo em: {detrate_bar_path}\")\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Gráfico 3: Gráfico de Barras da Taxa de Detecção por Landmark e Método\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    sns.barplot(data=summary_combined_df, x=\"Landmark\", y=\"DetectionRate\", hue=\"Method\")\n",
    "    plt.title(\"Taxa de Detecção (%) por Landmark e Método\")\n",
    "    plt.ylabel(\"Taxa de Detecção (%)\")\n",
    "    plt.xlabel(\"Landmark\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylim(0, 105)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    detrate_landmark_path = os.path.join(RESULTS_DIR, \"comparison_detection_rate_landmark_barplot.png\")\n",
    "    plt.savefig(detrate_landmark_path)\n",
    "    print(f\"Gráfico de Barras da Taxa de Detecção por Landmark salvo em: {detrate_landmark_path}\")\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "else:\n",
    "    print(\"Dados de sumário não disponíveis para gerar gráficos de taxa de detecção.\")"\n",
    "\n",
    "# Exibir imagens no notebook (opcional)\n",
    "# from IPython.display import Image, display\n",
    "# if os.path.exists(boxplot_path): display(Image(filename=boxplot_path))\n",
    "# if os.path.exists(detrate_bar_path): display(Image(filename=detrate_bar_path))\n",
    "# if os.path.exists(detrate_landmark_path): display(Image(filename=detrate_landmark_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análise de Complexidade Computacional (Tempo de Execução)\n",
    "\n",
    "Se o script `main.py` foi executado com logging de tempo (usando o decorador `@timeit` ou similar) e os logs foram salvos, podemos extrair informações sobre o tempo de processamento por arquivo e por método.\n",
    "\n",
    "**Nota:** Esta seção é mais conceitual, pois a extração e análise de logs não está implementada aqui. Seria necessário:\n",
    "1. Modificar `main.py` para salvar tempos de processamento em um arquivo estruturado (ex: CSV) ou garantir que os logs sejam facilmente parseáveis.\n",
    "2. Ler esses dados aqui e calcular tempos médios/totais por método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo conceitual de como analisar tempos (requer dados de tempo)\n",
    "logging.info(\"Análise de tempo de execução (conceitual).\")\n",
    "# Suponha que temos um DataFrame 'timing_df' com colunas [\"FileID\", \"Method\", \"ProcessingTime\"]\n",
    "# timing_df = pd.read_csv(os.path.join(RESULTS_DIR, \"processing_times.csv\")) # Exemplo\n",
    "\n",
    "# if 'timing_df' in locals() and not timing_df.empty:\n",
    "#     plt.figure(figsize=(8, 5))\n",
    "#     sns.boxplot(data=timing_df, x=\"Method\", y=\"ProcessingTime\")\n",
    "#     plt.title(\"Tempo de Processamento por Arquivo e Método\")\n",
    "#     plt.ylabel(\"Tempo (segundos)\")\n",
    "#     plt.xlabel(\"Método\")\n",
    "#     plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "#     plt.tight_layout()\n",
    "#     timing_boxplot_path = os.path.join(RESULTS_DIR, \"comparison_timing_boxplot.png\")\n",
    "#     plt.savefig(timing_boxplot_path)\n",
    "#     print(f\"Gráfico Boxplot de Tempo salvo em: {timing_boxplot_path}\")\n",
    "#     # plt.show()\n",
    "#     plt.close()\n",
    "# else:\n",
    "#     print(\"Dados de tempo de processamento não disponíveis para análise.\")\n",
    "print(\"Implementação da análise de tempo requer dados de timing gerados por main.py.\")"\n   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão da Análise\n",
    "\n",
    "Este notebook apresentou uma estrutura para avaliar e comparar quantitativamente os métodos de detecção de landmarks implementados.\n",
    "\n",
    "- **Métricas de Precisão:** Calculamos o erro de detecção (distância Euclidiana) para cada landmark e o erro médio (MDE) por arquivo. Os boxplots ajudam a visualizar a distribuição dos erros e identificar landmarks mais difíceis de detectar por cada método.\n",
    "- **Métricas de Robustez:** A taxa de detecção indica a porcentagem de vezes que um landmark foi encontrado com sucesso (quando o GT estava disponível). Gráficos de barras comparam a robustez geral e por landmark.\n",
    "- **Complexidade Computacional:** Embora não totalmente implementada aqui, a análise do tempo de execução é crucial para comparar a eficiência dos métodos, especialmente considerando o requisito de hardware limitado.\n",
    "\n",
    "Com base nos dados (dummy), podemos observar as diferenças esperadas: o método ML tende a ser mais preciso (menor erro) e robusto (maior taxa de detecção), enquanto o método geométrico é provavelmente mais rápido (análise de tempo necessária para confirmar). A escolha do melhor método para o TCC dependerá do balanço entre precisão, robustez e eficiência computacional desejado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

