{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√µes Iniciais e Imports\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import trimesh\n",
    "from pathlib import Path\n",
    "\n",
    "# Adicionar o diret√≥rio raiz do projeto ao path\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.utils.metrics import run_evaluation_on_dataset, load_landmarks_from_json\n",
    "from src.utils.helpers import setup_logging, list_stl_files, save_landmarks_to_json\n",
    "from src.core.landmarks import LANDMARK_NAMES\n",
    "\n",
    "# Configurar logging\n",
    "setup_logging(log_level=logging.INFO)\n",
    "\n",
    "# Configurar estilo dos gr√°ficos\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Diret√≥rio de trabalho: {os.getcwd()}\")\n",
    "print(f\"Path do m√≥dulo: {module_path}\")\n",
    "print(f\"Landmarks definidos: {len(LANDMARK_NAMES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b42fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar diret√≥rios\n",
    "BASE_DIR = module_path\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n",
    "RESULTS_GEOM_DIR = os.path.join(RESULTS_DIR, \"geometric\") \n",
    "RESULTS_ML_DIR = os.path.join(RESULTS_DIR, \"ml\")\n",
    "GROUND_TRUTH_DIR = os.path.join(BASE_DIR, \"data\", \"ground_truth\")\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"skulls\")\n",
    "\n",
    "# Criar diret√≥rios se n√£o existirem\n",
    "for directory in [RESULTS_GEOM_DIR, RESULTS_ML_DIR, GROUND_TRUTH_DIR, DATA_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(f\"Diret√≥rios configurados:\")\n",
    "print(f\"  Resultados gerais: {RESULTS_DIR}\")\n",
    "print(f\"  Resultados geom√©tricos: {RESULTS_GEOM_DIR}\")\n",
    "print(f\"  Resultados ML: {RESULTS_ML_DIR}\")\n",
    "print(f\"  Ground truth: {GROUND_TRUTH_DIR}\")\n",
    "print(f\"  Dados originais: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1eb0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar dados dummy para demonstra√ß√£o\n",
    "# Em um cen√°rio real, estes dados viriam de execu√ß√µes do main.py\n",
    "\n",
    "print(\"=== Criando Dados Dummy para Demonstra√ß√£o ===\")\n",
    "print(\"‚ö†Ô∏è  Em um cen√°rio real, estes dados seriam gerados pelo main.py\")\n",
    "\n",
    "DUMMY_FILE_IDS = [\"dummy_A\", \"dummy_B\", \"dummy_C\"]\n",
    "\n",
    "for file_id in DUMMY_FILE_IDS:\n",
    "    # 1. Criar arquivo STL dummy (se n√£o existir)\n",
    "    dummy_stl_path = os.path.join(DATA_DIR, f\"{file_id}.stl\")\n",
    "    if not os.path.exists(dummy_stl_path):\n",
    "        mesh_dummy = trimesh.primitives.Sphere(radius=50 + np.random.randint(10))\n",
    "        mesh_dummy.export(dummy_stl_path)\n",
    "        \n",
    "    # 2. Criar Ground Truth dummy com coordenadas realistas\n",
    "    gt_path = os.path.join(GROUND_TRUTH_DIR, f\"{file_id}_landmarks_gt.json\")\n",
    "    if not os.path.exists(gt_path):\n",
    "        # Gerar coordenadas GT realistas baseadas em anatomia\n",
    "        base_coords = {\n",
    "            \"Glabela\": [0, 50, 50],\n",
    "            \"Nasion\": [0, 45, 40], \n",
    "            \"Bregma\": [0, 0, 100],\n",
    "            \"Opisthocranion\": [0, -50, 50],\n",
    "            \"Euryon_Esquerdo\": [-50, 0, 50],\n",
    "            \"Euryon_Direito\": [50, 0, 50],\n",
    "            \"Vertex\": [0, 5, 100],\n",
    "            \"Inion\": [0, -45, 35]\n",
    "        }\n",
    "        \n",
    "        # Adicionar varia√ß√£o individual\n",
    "        gt_data = {}\n",
    "        for name, base_coord in base_coords.items():\n",
    "            # Adicionar varia√ß√£o de ¬±5mm em cada eixo\n",
    "            variation = (np.random.rand(3) - 0.5) * 10\n",
    "            gt_data[name] = (np.array(base_coord) + variation).tolist()\n",
    "        \n",
    "        # Simular alguns landmarks ausentes no GT\n",
    "        if file_id == \"dummy_C\":\n",
    "            gt_data[\"Vertex\"] = None\n",
    "            \n",
    "        with open(gt_path, \"w\") as f:\n",
    "            json.dump(gt_data, f, indent=4)\n",
    "    \n",
    "    # 3. Criar resultados geom√©tricos dummy (com erro simulado)\n",
    "    geom_pred_path = os.path.join(RESULTS_GEOM_DIR, f\"{file_id}_landmarks.json\")\n",
    "    if not os.path.exists(geom_pred_path):\n",
    "        gt_data = load_landmarks_from_json(gt_path)\n",
    "        if gt_data:\n",
    "            geom_pred = {}\n",
    "            for name, gt_coord in gt_data.items():\n",
    "                if gt_coord is not None and np.random.rand() > 0.15:  # 85% detec√ß√£o\n",
    "                    # Erro simulado: ¬±3mm com vi√©s\n",
    "                    error = (np.random.rand(3) - 0.5) * 6\n",
    "                    geom_pred[name] = (np.array(gt_coord) + error).tolist()\n",
    "                else:\n",
    "                    geom_pred[name] = None\n",
    "            \n",
    "            with open(geom_pred_path, \"w\") as f:\n",
    "                json.dump(geom_pred, f, indent=4)\n",
    "    \n",
    "    # 4. Criar resultados ML dummy (erro menor, melhor taxa)\n",
    "    ml_pred_path = os.path.join(RESULTS_ML_DIR, f\"{file_id}_landmarks.json\")\n",
    "    if not os.path.exists(ml_pred_path):\n",
    "        gt_data = load_landmarks_from_json(gt_path)\n",
    "        if gt_data:\n",
    "            ml_pred = {}\n",
    "            for name, gt_coord in gt_data.items():\n",
    "                if gt_coord is not None and np.random.rand() > 0.05:  # 95% detec√ß√£o\n",
    "                    # Erro menor: ¬±1.5mm\n",
    "                    error = (np.random.rand(3) - 0.5) * 3\n",
    "                    ml_pred[name] = (np.array(gt_coord) + error).tolist()\n",
    "                else:\n",
    "                    ml_pred[name] = None\n",
    "            \n",
    "            with open(ml_pred_path, \"w\") as f:\n",
    "                json.dump(ml_pred, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Dados dummy criados para {len(DUMMY_FILE_IDS)} arquivos\")\n",
    "print(f\"   Ground truth: {len(DUMMY_FILE_IDS)} arquivos\")\n",
    "print(f\"   Resultados geom√©tricos: {len(list(Path(RESULTS_GEOM_DIR).glob('*.json')))} arquivos\")\n",
    "print(f\"   Resultados ML: {len(list(Path(RESULTS_ML_DIR).glob('*.json')))} arquivos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c9b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Executando Avalia√ß√£o dos M√©todos ===\")\n",
    "\n",
    "# Avalia√ß√£o do M√©todo Geom√©trico\n",
    "print(\"\\nüîç Avaliando M√©todo Geom√©trico...\")\n",
    "results_geom_df, summary_geom_df = run_evaluation_on_dataset(\n",
    "    results_dir=RESULTS_GEOM_DIR, \n",
    "    ground_truth_dir=GROUND_TRUTH_DIR, \n",
    "    method_name=\"Geometric\"\n",
    ")\n",
    "\n",
    "if not results_geom_df.empty:\n",
    "    print(f\"‚úÖ Avalia√ß√£o geom√©trica conclu√≠da: {len(results_geom_df)} registros\")\n",
    "    overall_geom_rate = results_geom_df[\"Detected\"].mean() * 100\n",
    "    overall_geom_error = results_geom_df[\"Error\"].mean()\n",
    "    print(f\"   Taxa de detec√ß√£o: {overall_geom_rate:.1f}%\")\n",
    "    print(f\"   Erro m√©dio: {overall_geom_error:.3f} mm\")\n",
    "else:\n",
    "    print(\"‚ùå Avalia√ß√£o geom√©trica falhou\")\n",
    "\n",
    "# Avalia√ß√£o do M√©todo ML\n",
    "print(\"\\nüîç Avaliando M√©todo Machine Learning...\")\n",
    "results_ml_df, summary_ml_df = run_evaluation_on_dataset(\n",
    "    results_dir=RESULTS_ML_DIR, \n",
    "    ground_truth_dir=GROUND_TRUTH_DIR, \n",
    "    method_name=\"ML\"\n",
    ")\n",
    "\n",
    "if not results_ml_df.empty:\n",
    "    print(f\"‚úÖ Avalia√ß√£o ML conclu√≠da: {len(results_ml_df)} registros\")\n",
    "    overall_ml_rate = results_ml_df[\"Detected\"].mean() * 100\n",
    "    overall_ml_error = results_ml_df[\"Error\"].mean()\n",
    "    print(f\"   Taxa de detec√ß√£o: {overall_ml_rate:.1f}%\")\n",
    "    print(f\"   Erro m√©dio: {overall_ml_error:.3f} mm\")\n",
    "else:\n",
    "    print(\"‚ùå Avalia√ß√£o ML falhou\")\n",
    "\n",
    "# Combinar resultados para an√°lise comparativa\n",
    "results_combined_df = pd.DataFrame()\n",
    "summary_combined_df = pd.DataFrame()\n",
    "\n",
    "if not results_geom_df.empty and not results_ml_df.empty:\n",
    "    results_combined_df = pd.concat([results_geom_df, results_ml_df], ignore_index=True)\n",
    "    summary_combined_df = pd.concat([summary_geom_df, summary_ml_df], ignore_index=True)\n",
    "    print(f\"\\nüìä Datasets combinados criados:\")\n",
    "    print(f\"   Resultados detalhados: {len(results_combined_df)} registros\")\n",
    "    print(f\"   Resumo por landmark: {len(summary_combined_df)} registros\")\n",
    "elif not results_geom_df.empty:\n",
    "    results_combined_df = results_geom_df\n",
    "    summary_combined_df = summary_geom_df\n",
    "    print(\"‚ö†Ô∏è  Apenas resultados geom√©tricos dispon√≠veis\")\n",
    "elif not results_ml_df.empty:\n",
    "    results_combined_df = results_ml_df\n",
    "    summary_combined_df = summary_ml_df\n",
    "    print(\"‚ö†Ô∏è  Apenas resultados ML dispon√≠veis\")\n",
    "else:\n",
    "    print(\"‚ùå Nenhum resultado v√°lido para an√°lise\")\n",
    "\n",
    "# Salvar datasets combinados\n",
    "if not results_combined_df.empty:\n",
    "    results_csv = os.path.join(RESULTS_DIR, \"evaluation_combined_detailed.csv\")\n",
    "    summary_csv = os.path.join(RESULTS_DIR, \"evaluation_combined_summary.csv\")\n",
    "    \n",
    "    results_combined_df.to_csv(results_csv, index=False)\n",
    "    summary_combined_df.to_csv(summary_csv, index=False)\n",
    "    \n",
    "    print(f\"\\nüíæ Resultados salvos:\")\n",
    "    print(f\"   Detalhados: {results_csv}\")\n",
    "    print(f\"   Resumo: {summary_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ea802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir resumo dos resultados\n",
    "if not results_combined_df.empty:\n",
    "    print(\"=== Resumo dos Resultados Detalhados ===\")\n",
    "    print(results_combined_df.head(10).round(3))\n",
    "    \n",
    "if not summary_combined_df.empty:\n",
    "    print(\"\\n=== Resumo Agregado por Landmark ===\")\n",
    "    # Formatar colunas num√©ricas\n",
    "    display_summary = summary_combined_df.copy()\n",
    "    numeric_cols = ['MeanError', 'StdError', 'MedianError', 'MinError', 'MaxError', 'DetectionRate']\n",
    "    for col in numeric_cols:\n",
    "        if col in display_summary.columns:\n",
    "            if col == 'DetectionRate':\n",
    "                display_summary[col] = display_summary[col].round(1).astype(str) + '%'\n",
    "            else:\n",
    "                display_summary[col] = display_summary[col].round(3)\n",
    "    \n",
    "    print(display_summary[['Landmark', 'MeanError', 'DetectionRate', 'NumDetected', 'NumGT']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0169ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Gerando Visualiza√ß√µes Comparativas ===\")\n",
    "\n",
    "if not results_combined_df.empty:\n",
    "    # Preparar dados para visualiza√ß√£o (remover NaN para os gr√°ficos)\n",
    "    plot_data = results_combined_df.dropna(subset=['Error'])\n",
    "    \n",
    "    if len(plot_data) > 0:\n",
    "        # Configurar subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('An√°lise Comparativa dos M√©todos de Detec√ß√£o', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Gr√°fico 1: Boxplot do Erro por M√©todo\n",
    "        ax1 = axes[0, 0]\n",
    "        sns.boxplot(data=plot_data, x=\"Method\", y=\"Error\", ax=ax1)\n",
    "        ax1.set_title(\"Distribui√ß√£o de Erros por M√©todo\")\n",
    "        ax1.set_ylabel(\"Erro de Detec√ß√£o (mm)\")\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Gr√°fico 2: Boxplot do Erro por Landmark\n",
    "        ax2 = axes[0, 1]\n",
    "        # Selecionar apenas landmarks com dados suficientes\n",
    "        landmark_counts = plot_data['Landmark'].value_counts()\n",
    "        popular_landmarks = landmark_counts[landmark_counts >= 2].index[:6]  # Top 6\n",
    "        \n",
    "        if len(popular_landmarks) > 0:\n",
    "            landmark_data = plot_data[plot_data['Landmark'].isin(popular_landmarks)]\n",
    "            sns.boxplot(data=landmark_data, x=\"Landmark\", y=\"Error\", ax=ax2)\n",
    "            ax2.set_title(\"Distribui√ß√£o de Erros por Landmark\")\n",
    "            ax2.set_ylabel(\"Erro de Detec√ß√£o (mm)\")\n",
    "            ax2.tick_params(axis='x', rotation=45)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'Dados insuficientes\\npara landmarks', \n",
    "                    ha='center', va='center', transform=ax2.transAxes)\n",
    "            ax2.set_title(\"Distribui√ß√£o por Landmark\")\n",
    "        \n",
    "        # Gr√°fico 3: Taxa de Detec√ß√£o por M√©todo\n",
    "        ax3 = axes[1, 0]\n",
    "        if not summary_combined_df.empty:\n",
    "            method_summary = summary_combined_df.groupby('Method')['DetectionRate'].mean().reset_index()\n",
    "            method_summary['Method'] = method_summary['Method'].fillna('Unknown')\n",
    "            \n",
    "            bars = sns.barplot(data=method_summary, x=\"Method\", y=\"DetectionRate\", ax=ax3)\n",
    "            ax3.set_title(\"Taxa de Detec√ß√£o M√©dia por M√©todo\")\n",
    "            ax3.set_ylabel(\"Taxa de Detec√ß√£o (%)\")\n",
    "            ax3.set_ylim(0, 105)\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Adicionar valores nas barras\n",
    "            for i, bar in enumerate(bars.patches):\n",
    "                height = bar.get_height()\n",
    "                if not np.isnan(height):\n",
    "                    ax3.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                           f'{height:.1f}%', ha='center', va='bottom')\n",
    "        else:\n",
    "            ax3.text(0.5, 0.5, 'Dados de resumo\\nn√£o dispon√≠veis', \n",
    "                    ha='center', va='center', transform=ax3.transAxes)\n",
    "            ax3.set_title(\"Taxa de Detec√ß√£o por M√©todo\")\n",
    "        \n",
    "        # Gr√°fico 4: Scatter Error vs Landmark para ambos m√©todos\n",
    "        ax4 = axes[1, 1]\n",
    "        if 'Method' in plot_data.columns and len(plot_data['Method'].unique()) > 1:\n",
    "            for method in plot_data['Method'].unique():\n",
    "                method_data = plot_data[plot_data['Method'] == method]\n",
    "                ax4.scatter(range(len(method_data)), method_data['Error'], \n",
    "                          label=method, alpha=0.6, s=30)\n",
    "            \n",
    "            ax4.set_title(\"Erro de Detec√ß√£o por Amostra\")\n",
    "            ax4.set_xlabel(\"√çndice da Amostra\")\n",
    "            ax4.set_ylabel(\"Erro (mm)\")\n",
    "            ax4.legend()\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax4.scatter(range(len(plot_data)), plot_data['Error'], alpha=0.6)\n",
    "            ax4.set_title(\"Erro de Detec√ß√£o por Amostra\")\n",
    "            ax4.set_xlabel(\"√çndice da Amostra\")\n",
    "            ax4.set_ylabel(\"Erro (mm)\")\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Salvar gr√°fico\n",
    "        comparison_plot_path = os.path.join(RESULTS_DIR, \"comparison_analysis.png\")\n",
    "        plt.savefig(comparison_plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Gr√°fico de an√°lise comparativa salvo em: {comparison_plot_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå N√£o h√° dados v√°lidos (sem NaN) para gerar gr√°ficos\")\n",
    "else:\n",
    "    print(\"‚ùå Dados combinados n√£o dispon√≠veis para visualiza√ß√£o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e84088",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== An√°lise Estat√≠stica Detalhada ===\")\n",
    "\n",
    "if not results_combined_df.empty:\n",
    "    # Estat√≠sticas gerais por m√©todo\n",
    "    print(\"\\nüìä Estat√≠sticas Gerais por M√©todo:\")\n",
    "    \n",
    "    for method in results_combined_df['Method'].unique():\n",
    "        method_data = results_combined_df[results_combined_df['Method'] == method]\n",
    "        valid_errors = method_data['Error'].dropna()\n",
    "        \n",
    "        if len(valid_errors) > 0:\n",
    "            print(f\"\\n{method}:\")\n",
    "            print(f\"  Total de detec√ß√µes: {len(method_data)}\")\n",
    "            print(f\"  Detec√ß√µes v√°lidas: {len(valid_errors)} ({len(valid_errors)/len(method_data)*100:.1f}%)\")\n",
    "            print(f\"  Erro m√©dio: {valid_errors.mean():.3f} ¬± {valid_errors.std():.3f} mm\")\n",
    "            print(f\"  Erro mediano: {valid_errors.median():.3f} mm\")\n",
    "            print(f\"  Erro m√≠nimo: {valid_errors.min():.3f} mm\")\n",
    "            print(f\"  Erro m√°ximo: {valid_errors.max():.3f} mm\")\n",
    "            print(f\"  Percentil 95: {valid_errors.quantile(0.95):.3f} mm\")\n",
    "        else:\n",
    "            print(f\"\\n{method}: Sem erros v√°lidos para an√°lise\")\n",
    "    \n",
    "    # An√°lise por landmark\n",
    "    if not summary_combined_df.empty:\n",
    "        print(\"\\nüéØ Top Landmarks por Taxa de Detec√ß√£o:\")\n",
    "        \n",
    "        # Agrupar por landmark (m√©dia entre m√©todos)\n",
    "        landmark_avg = summary_combined_df.groupby('Landmark').agg({\n",
    "            'DetectionRate': 'mean',\n",
    "            'MeanError': 'mean',\n",
    "            'NumDetected': 'sum',\n",
    "            'NumGT': 'sum'\n",
    "        }).round(3)\n",
    "        \n",
    "        # Ordenar por taxa de detec√ß√£o\n",
    "        landmark_avg_sorted = landmark_avg.sort_values('DetectionRate', ascending=False)\n",
    "        \n",
    "        print(landmark_avg_sorted.head(8))\n",
    "        \n",
    "        # Landmarks mais desafiadores\n",
    "        print(\"\\n‚ö†Ô∏è  Landmarks Mais Desafiadores (baixa taxa de detec√ß√£o):\")\n",
    "        challenging = landmark_avg_sorted.tail(3)\n",
    "        for landmark, data in challenging.iterrows():\n",
    "            print(f\"  {landmark}: {data['DetectionRate']:.1f}% detec√ß√£o, erro {data['MeanError']:.3f}mm\")\n",
    "        \n",
    "        # Landmarks mais precisos\n",
    "        print(\"\\n‚úÖ Landmarks Mais Precisos (menor erro):\")\n",
    "        precise = landmark_avg.dropna(subset=['MeanError']).sort_values('MeanError').head(3)\n",
    "        for landmark, data in precise.iterrows():\n",
    "            print(f\"  {landmark}: {data['MeanError']:.3f}mm erro, {data['DetectionRate']:.1f}% detec√ß√£o\")\n",
    "\n",
    "    # Compara√ß√£o estat√≠stica entre m√©todos (se ambos dispon√≠veis)\n",
    "    methods = results_combined_df['Method'].unique()\n",
    "    if len(methods) >= 2:\n",
    "        print(\"\\nüîç Compara√ß√£o Estat√≠stica entre M√©todos:\")\n",
    "        \n",
    "        method1_errors = results_combined_df[results_combined_df['Method'] == methods[0]]['Error'].dropna()\n",
    "        method2_errors = results_combined_df[results_combined_df['Method'] == methods[1]]['Error'].dropna()\n",
    "        \n",
    "        if len(method1_errors) > 0 and len(method2_errors) > 0:\n",
    "            # Teste t (assumindo normalidade)\n",
    "            from scipy import stats\n",
    "            t_stat, p_value = stats.ttest_ind(method1_errors, method2_errors)\n",
    "            \n",
    "            print(f\"  Diferen√ßa de m√©dias: {method1_errors.mean() - method2_errors.mean():.3f} mm\")\n",
    "            print(f\"  Teste t: t={t_stat:.3f}, p={p_value:.4f}\")\n",
    "            \n",
    "            if p_value < 0.05:\n",
    "                better_method = methods[0] if method1_errors.mean() < method2_errors.mean() else methods[1]\n",
    "                print(f\"  ‚úÖ {better_method} √© significativamente melhor (p < 0.05)\")\n",
    "            else:\n",
    "                print(f\"  ‚öñÔ∏è  N√£o h√° diferen√ßa significativa entre os m√©todos (p >= 0.05)\")\n",
    "        else:\n",
    "            print(\"  Dados insuficientes para compara√ß√£o estat√≠stica\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Dados n√£o dispon√≠veis para an√°lise estat√≠stica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19580f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Relat√≥rio de Performance por Landmark ===\")\n",
    "\n",
    "if not summary_combined_df.empty:\n",
    "    # Criar relat√≥rio por landmark\n",
    "    landmark_report = []\n",
    "    \n",
    "    for landmark in LANDMARK_NAMES:\n",
    "        landmark_data = summary_combined_df[summary_combined_df['Landmark'] == landmark]\n",
    "        \n",
    "        if not landmark_data.empty:\n",
    "            # Calcular m√©tricas m√©dias entre m√©todos\n",
    "            avg_detection_rate = landmark_data['DetectionRate'].mean()\n",
    "            avg_error = landmark_data['MeanError'].mean()\n",
    "            total_detected = landmark_data['NumDetected'].sum()\n",
    "            total_gt = landmark_data['NumGT'].sum()\n",
    "            \n",
    "            # Classificar dificuldade\n",
    "            if avg_detection_rate >= 90:\n",
    "                difficulty = \"F√°cil üòä\"\n",
    "            elif avg_detection_rate >= 70:\n",
    "                difficulty = \"Moderado üòê\"\n",
    "            else:\n",
    "                difficulty = \"Dif√≠cil üòì\"\n",
    "            \n",
    "            # Classificar precis√£o\n",
    "            if not np.isnan(avg_error):\n",
    "                if avg_error <= 2.0:\n",
    "                    precision = \"Alta ‚úÖ\"\n",
    "                elif avg_error <= 5.0:\n",
    "                    precision = \"M√©dia ‚ö†Ô∏è\"\n",
    "                else:\n",
    "                    precision = \"Baixa ‚ùå\"\n",
    "                error_str = f\"{avg_error:.2f}mm\"\n",
    "            else:\n",
    "                precision = \"N/A\"\n",
    "                error_str = \"N/A\"\n",
    "            \n",
    "            landmark_report.append({\n",
    "                'Landmark': landmark,\n",
    "                'Taxa_Detec√ß√£o': f\"{avg_detection_rate:.1f}%\",\n",
    "                'Erro_M√©dio': error_str,\n",
    "                'Dificuldade': difficulty,\n",
    "                'Precis√£o': precision,\n",
    "                'Detec√ß√µes': f\"{total_detected}/{total_gt}\"\n",
    "            })\n",
    "    \n",
    "    # Criar DataFrame do relat√≥rio\n",
    "    if landmark_report:\n",
    "        report_df = pd.DataFrame(landmark_report)\n",
    "        \n",
    "        print(\"\\nüìã Relat√≥rio Resumido por Landmark:\")\n",
    "        print(report_df.to_string(index=False))\n",
    "        \n",
    "        # Salvar relat√≥rio\n",
    "        report_path = os.path.join(RESULTS_DIR, \"landmark_performance_report.csv\")\n",
    "        report_df.to_csv(report_path, index=False)\n",
    "        print(f\"\\nüíæ Relat√≥rio salvo em: {report_path}\")\n",
    "        \n",
    "        # Estat√≠sticas do relat√≥rio\n",
    "        easy_count = sum(1 for item in landmark_report if \"F√°cil\" in item['Dificuldade'])\n",
    "        moderate_count = sum(1 for item in landmark_report if \"Moderado\" in item['Dificuldade'])\n",
    "        hard_count = sum(1 for item in landmark_report if \"Dif√≠cil\" in item['Dificuldade'])\n",
    "        \n",
    "        print(f\"\\nüìä Distribui√ß√£o de Dificuldade:\")\n",
    "        print(f\"   F√°ceis: {easy_count} landmarks\")\n",
    "        print(f\"   Moderados: {moderate_count} landmarks\")\n",
    "        print(f\"   Dif√≠ceis: {hard_count} landmarks\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Nenhum dado v√°lido encontrado para gerar relat√≥rio\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Dados de resumo n√£o dispon√≠veis para gerar relat√≥rio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf156fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== An√°lise Final e Recomenda√ß√µes ===\")\n",
    "\n",
    "if not results_combined_df.empty:\n",
    "    # M√©tricas gerais finais\n",
    "    total_detections = len(results_combined_df)\n",
    "    successful_detections = len(results_combined_df.dropna(subset=['Error']))\n",
    "    overall_success_rate = (successful_detections / total_detections) * 100\n",
    "    overall_mean_error = results_combined_df['Error'].mean()\n",
    "    \n",
    "    print(f\"\\nüìä M√©tricas Gerais do Sistema:\")\n",
    "    print(f\"   Total de tentativas de detec√ß√£o: {total_detections:,}\")\n",
    "    print(f\"   Detec√ß√µes bem-sucedidas: {successful_detections:,}\")\n",
    "    print(f\"   Taxa de sucesso geral: {overall_success_rate:.1f}%\")\n",
    "    print(f\"   Erro m√©dio geral: {overall_mean_error:.3f} mm\")\n",
    "    \n",
    "    # Avalia√ß√£o da qualidade\n",
    "    print(f\"\\nüéØ Avalia√ß√£o da Qualidade:\")\n",
    "    if overall_success_rate >= 80:\n",
    "        quality_detection = \"Excelente ‚úÖ\"\n",
    "    elif overall_success_rate >= 60:\n",
    "        quality_detection = \"Boa ‚ö†Ô∏è\"\n",
    "    else:\n",
    "        quality_detection = \"Precisa melhorar ‚ùå\"\n",
    "    \n",
    "    if not np.isnan(overall_mean_error):\n",
    "        if overall_mean_error <= 3.0:\n",
    "            quality_precision = \"Excelente ‚úÖ\"\n",
    "        elif overall_mean_error <= 5.0:\n",
    "            quality_precision = \"Boa ‚ö†Ô∏è\"\n",
    "        else:\n",
    "            quality_precision = \"Precisa melhorar ‚ùå\"\n",
    "    else:\n",
    "        quality_precision = \"N√£o avali√°vel\"\n",
    "    \n",
    "    print(f\"   Taxa de detec√ß√£o: {quality_detection}\")\n",
    "    print(f\"   Precis√£o: {quality_precision}\")\n",
    "    \n",
    "    # Recomenda√ß√µes espec√≠ficas\n",
    "    print(f\"\\nüí° Recomenda√ß√µes para Melhoria:\")\n",
    "    \n",
    "    if overall_success_rate < 80:\n",
    "        print(f\"   üîß Taxa de detec√ß√£o baixa - considere:\")\n",
    "        print(f\"      ‚Ä¢ Refinar heur√≠sticas geom√©tricas\")\n",
    "        print(f\"      ‚Ä¢ Treinar modelos ML com mais dados\")\n",
    "        print(f\"      ‚Ä¢ Ajustar par√¢metros de confian√ßa\")\n",
    "    \n",
    "    if not np.isnan(overall_mean_error) and overall_mean_error > 3.0:\n",
    "        print(f\"   üéØ Precis√£o baixa - considere:\")\n",
    "        print(f\"      ‚Ä¢ Melhorar qualidade da simplifica√ß√£o\")\n",
    "        print(f\"      ‚Ä¢ Adicionar features mais discriminativas no ML\")\n",
    "        print(f\"      ‚Ä¢ Validar orienta√ß√£o e escala das malhas\")\n",
    "    \n",
    "    # Compara√ß√£o entre m√©todos\n",
    "    methods = results_combined_df['Method'].unique()\n",
    "    if len(methods) >= 2:\n",
    "        print(f\"\\n‚öñÔ∏è  Compara√ß√£o entre M√©todos:\")\n",
    "        for method in methods:\n",
    "            method_data = results_combined_df[results_combined_df['Method'] == method]\n",
    "            method_rate = method_data['Detected'].mean() * 100\n",
    "            method_error = method_data['Error'].mean()\n",
    "            print(f\"   {method}: {method_rate:.1f}% detec√ß√£o, {method_error:.3f}mm erro\")\n",
    "        \n",
    "        # Recomendar melhor m√©todo\n",
    "        method_scores = {}\n",
    "        for method in methods:\n",
    "            method_data = results_combined_df[results_combined_df['Method'] == method]\n",
    "            rate = method_data['Detected'].mean() * 100\n",
    "            error = method_data['Error'].mean()\n",
    "            # Score combinado (rate ponderado por precis√£o)\n",
    "            if not np.isnan(error):\n",
    "                score = rate * (1 / (1 + error))\n",
    "            else:\n",
    "                score = rate * 0.5  # Penalizar falta de dados\n",
    "            method_scores[method] = score\n",
    "        \n",
    "        best_method = max(method_scores, key=method_scores.get)\n",
    "        print(f\"\\nüèÜ M√©todo Recomendado: {best_method}\")\n",
    "        print(f\"   Com base na combina√ß√£o de taxa de detec√ß√£o e precis√£o\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Dados insuficientes para an√°lise final\")\n",
    "\n",
    "print(f\"\\n‚úÖ An√°lise completa conclu√≠da!\")\n",
    "print(f\"üìÅ Todos os resultados foram salvos em: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1766999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CONCLUS√ïES DA AN√ÅLISE ===\")\n",
    "print()\n",
    "print(\"Esta an√°lise quantitativa demonstrou a capacidade de avalia√ß√£o sistem√°tica\")\n",
    "print(\"dos m√©todos de detec√ß√£o implementados.\")\n",
    "print()\n",
    "print(\"### Principais Contribui√ß√µes:\")\n",
    "print()\n",
    "print(\"1. **Framework de Avalia√ß√£o Robusto**: Sistema completo para calcular m√©tricas de precis√£o e robustez\")\n",
    "print(\"2. **An√°lise Comparativa**: Compara√ß√£o objetiva entre m√©todos geom√©trico e ML\")\n",
    "print(\"3. **Visualiza√ß√µes Informativas**: Gr√°ficos que facilitam interpreta√ß√£o dos resultados\")\n",
    "print(\"4. **Relat√≥rios Automatizados**: Documenta√ß√£o estruturada da performance por landmark\")\n",
    "print(\"5. **Recomenda√ß√µes Baseadas em Dados**: Sugest√µes concretas para melhorias\")\n",
    "print()\n",
    "print(\"### Aspectos T√©cnicos Validados:\")\n",
    "print()\n",
    "print(\"- ‚úÖ **M√©tricas Implementadas**: Erro de detec√ß√£o, MDE, taxa de detec√ß√£o\")\n",
    "print(\"- ‚úÖ **Processamento em Lote**: Avalia√ß√£o automatizada de datasets\")\n",
    "print(\"- ‚úÖ **An√°lise Estat√≠stica**: Compara√ß√µes significativas entre m√©todos\")\n",
    "print(\"- ‚úÖ **Visualiza√ß√£o Profissional**: Gr√°ficos publication-ready\")\n",
    "print(\"- ‚úÖ **Relat√≥rios Estruturados**: CSV e visualiza√ß√µes para documenta√ß√£o\")\n",
    "print()\n",
    "print(\"### Para Trabalhos Futuros:\")\n",
    "print()\n",
    "print(\"Este framework de an√°lise est√° pronto para:\")\n",
    "print(\"- Avalia√ß√£o com dados reais (MUG500+, NMDID)\")\n",
    "print(\"- Compara√ß√£o com m√©todos da literatura\")\n",
    "print(\"- Otimiza√ß√£o de hiperpar√¢metros baseada em m√©tricas\")\n",
    "print(\"- Valida√ß√£o cruzada e testes estat√≠sticos rigorosos\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
